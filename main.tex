\documentclass[conference]{IEEEtran} % or another class, depending on your target conference

% Packages
\usepackage{graphicx} % For including images
\usepackage{amsmath}  % For mathematical formulas
\usepackage{hyperref} % For hyperlinks
\usepackage{cite}     % For handling citations
\usepackage{caption}  % For customizing captions


% Title
\title{Multi-Step Object Re-Identification on Edge Devices: A Pipeline for Vehicle Re-Identification}

% Author(s)
\author{
	\IEEEauthorblockN{Tomass Zutis, Anzelika Bureka, Janis Judvaitis, Janis Arents, Modris Greitans, Peteris Racinskis}
	\IEEEauthorblockA{
		Institute of Electronics and Computer Science, Latvia\\
		janis.arents@edi.lv
	}
}

% Document
\begin{document}
	
	% Title page
	\maketitle
	
	% Abstract
	\begin{abstract}
A method that leverages a multi-step process focused on extracting and using object features for object re-identification is described. The proposed pipeline includes steps such as: detecting an object, converting its features into a vector embedding, storing this embedding in a vector database, and then querying the database to find the same or similar objects based on their feature embeddings. This approach allows for the identification of the same object across different images or cameras, even in varying locations, such as in Vehicle Re-Identification scenarios. For situations where re-identification needs to happen in outdoor environments or on-the-go, implementing this process on edge devices becomes crucial. Therefore, multiple ways to tailor the pipeline and its outputs for edge devices are outlined and investigated. The presentation provides a detailed explanation of the pipelineâ€™s structure and how it functions on edge devices, along with the experimental setup demonstrating its application, particularly in vehicle re-identification.
	\end{abstract}
	
	% Keywords
	\begin{IEEEkeywords}
		Re-Identification, Feature extractions, Vector embeddings, Neural networks, Edge devices, Vector 
		databases
	\end{IEEEkeywords}
	
	% Sections
	\section{Introduction}
%	Introduce the problem of vehicle re-identification and the challenges of implementing this on edge devices. Discuss the significance of efficient computation in real-time applications.

Monitoring and recognizing objects in photos and videos has long been a field of interest for many. It has also become a well researched topic ever since computer vision has accelerated it's possibilities, in the last decade especially  \cite{prince2012computer}.
% vajag rakstu par computer vision potential un possibilities, lkm jau vecaaku
 With computer vision came image classification, which helped us classify images based on the object depicted. That gave us the answer to the question: "What is the object in the image?" \cite{lu2007survey}.
 % Te rakstu par object classification
  Later on came great progress in object detection. That could answer the questions of: "Where are the objects in the image?" and "What are the objects in the image". These were great leaps forward if the job was to pay attention to only objects of a specific class and there were multiple of them in a frame. This was first and foremost fostered by the advancement of the Convolutional Neural Network (CNN) and its variants. \cite{du2018understanding}.
  
% Te varetu ieklaut atsauci uz kadu pirmo sasniegumu objektu detektesanaa un pateikt ka vot vini domaja ka tas bija sasniegums
 In this article, however, the authors are looking for a solution that could see these objects of a single class and distinguish between them, using the specific features that exist only for a single individual in a class.
% Te laikam vajag ielikt definiciju no kaada raksta:
 This problem is called object Re-Identification - widely regarded as a sub-problem of image retrieval \cite{li2019object}.In imagery from at least two different cameras, object Re-identification aims to correctly label the same individual after imaging conditions (like scene, lighting conditions, object pose and others) have changed. In fact, this could be repeated for a specific class in any number of scenes, where there are usually more of these class objects visible. These could be images of vehicles, people or even animals or simple in-animate objects that have great intra-class variations.
 
 We are also in a unique position where there is still a lot of research to be done on re-identification itself, however, at the same time we are faced with the need to tailor these solutions for edge computing to preserve relevance in global technology and research trends. This is because some of the most common applications of re-identification are in traffic and security that use network cameras and cloud computing \cite{barthelemy2019edge} \cite{wang2024efficient}.
 Cities are looking for edge-computing devices using computer vision and deep neural networks to track real-time events in the public space. With the development of the Internet of Everything (IoE), the number of smart devices connected to the internet, the volume of available video footage, and the influx of sensory data have made the large-scale accumulation of big data inevitable \cite{cao2020overview}. This is why fully automated systems are needed to process data and re-identify patterns and objects in the smart cities environment, for no human or group of humans can be employed to process all this data manually and more crucially - in real time.
 
 \begin{figure}[b]
 	\centering
 	\includegraphics[width=0.5\textwidth]{re_id_diagramma_1.png} % Adjust width as needed
 	\caption{There are multiple uses for object re-identification in the smart cities context. Some of them are: people, vehicles, animals and even luggage in transit hubs.}
 	\label{fig:fig1} % Label for referencing the image
 \end{figure}
 
 In the scenario of traffic monitoring, there is the need to re-identify vehicles on the go to: first of all, be able to track them in a road network and, second, model future traffic based on the existing patterns \cite{barthelemy2019edge}. Edge computing proposes a hardware and software solution that would do this in real time - live video feed would be received from a network camera, that's deployed in the area of interest and the video analytics (i.e. detection and re-identification algorithms) are run directly on the edge device and only the results of the processing are transmitted.
 
 The natural solution to the object re-identification problem in these circumstances is a pipeline consisting of algorithms designed to cover all the necessary steps for re-identification to work. We argue that constructing a pipeline that receives live video feed, processes the frames to extract objects from each frame, saves these objects into memory and recognizes the same objects in a different scene's live video feed is possible and we aim to describe it in detail.
 
	
	\section{Related Work and State of the Art}
	
Object re-identification to this day is heavily reliant on extracting robust feature representations for the objects we are trying to save. Differences in lighting, angle, occlusions, multiple models of the same object (vehicle) are an obstacle in getting reliable predictions \cite{kuma2019vehicle}. However, there is more to a working re-identification pipeline than just looking for the best way to produce vector embeddings. We have looked at the state of the art for multiple components of this problem, such ass: Object detection, Object re-identification and feature extractions, Person re-Identification, Vehicle re-identification, Available datasets, Synthetic datasets and Edge implementation.
	
	\subsection{Object detection}
	
		Convolutional neural networks (CNNs) have been incredibly incremental in computer vision tasks, including object detection. YOLO - the "You Only Look Once" model is one of the best performing and regularly maintained choices. The latest YOLO v8 version (in the authors opinion v8 is the latest model with widespread and public support) has shown significant improvements in accuracy and speed \cite{Rath2023online}, which is crucial for real-time applications like ours. YOLO v8 leverages the experience from the previous YOLO versions, having around 10\% better mean average precision in object detection than the previously popular v5 (when comparing medium model size).
	
	\subsection{Object feature extraction}
	
		Feature extraction is used to extract the most distinct features from an image, which is used to describe it. We try to save these features in a low dimensional vector space \cite{salau2019feature}. Before image feature extraction, multiple pre-processing stages are usually employed like normalization, thresholding, binarization, resizing and others. We can expect a model to extract: color, texture, shape, motion and localization features. However, when dealing with one class objects with small intra-class variations specific types of features can be learned by the model like face features in the case of facial recognition.
		
		As of 2024 CNNs have become predominant choices in object features extraction thanks to their strong representation power and the ability to learn deep invariant embeddings \cite{zheng2019joint}.
	
	\subsection{Vehicle re-identification}
	
		It is the case that in re-identification, there seems to be a better tailored model for each benchmark out there. For example, in the realm of Vehicle Re-ID benchmarks MBR4B-LAI model by Almeida \textit{et al.} \cite{almeida2023strength} tops the VeRi-776 benchmark, "A strong baseline" model by Huynh \textit{et al.} \cite{huynh2021strong} tops the CityFlow benchmark and the vehiclenet model by Zheng \textit{et al.} \cite{zheng2019vehiclenet} is best at the VeRi benchmark. However we have chosen to pay particular interest to the paper by Zheng \textit{et al.} because of the baseline model that is applicable to all types of object re-identification and feature extraction. In \cite{zheng2017discriminatively} Zheng \textit{et al.} first introduces us to their baseline model and its architecture and demonstrates its capabilities specifically in pedestrian re-identification. In further papers, however, Zheng \textit{et al.} demonstrate tailoring of this baseline to vehicle re-identification \cite{zheng2019vehiclenet}, \cite{zheng2020vehiclenet} and person re-identification \cite{zheng2019joint}. We underline this baseline models usefulness by the versatility of its use in publications, its open code base and customizability and it's entry into most benchmarks. Even though it tops only one of them, it always keeps close to the top in terms of the Rank1 precision, according to the benchmark results on Papers with Code \cite{paperswithcode2024reid}. The model is implemented in Pytorch and is based on ResNet50 pre-trained on ImageNet, although this backbone is customizable.
	
		We will further refer to this model of interest as "baseline model by Zheng \textit{et al.}".
	
	\subsection{Person re-identification}
	
		The goal of Person re-identification is to match a person's visual identity across many different cameras or locations in video or image sequences. The SOTA in person re-identification can also be judged by the benchmark leaderboards available on Papers with code \cite{paperswithcode2024Persreid}. Some of the most important datasets-turned-benchmarks are: the Market-1501, where the current best Rank-1 accuracy is 98.27\% by Sharma \textit{et al.} \cite{sharma2021person}; the DukeMTMC-reID where the current best Rank-1 accuracy is 95.6\% by Wieczorek, \textit{et al.} \cite{wieczorek2021unreasonable}; and the CUHK03 where the current best Rank-1 accuracy is 98.7\% by by Sharma \textit{et al.} \cite{sharma2021person}. A modified version of the baseline model by Zheng \textit{et al.} has made an entry in the CUHK03 leaderboard 6th place in Rank-1 accuracy with 89.63\% and 3rd place in Rank-5 accuracy with 99.01\%.
		
	\subsection{Available datasets}
	
		One of the most ample vehicle detection datasets - UA-DETRAC comes from Wen \textit{et al.}  \cite{wen2020ua}. The UA-DETRAC benchmark dataset consists of 100 challenging video sequences captured from real-world traffic scenes. It can be used for object detection model training or fine-tuning.
		
		Some of the vehicle or person Re-ID datasets are already previously mentioned in the context of their usage in benchmarks. The VehicleID (PKU VehicleID) dataset has been introduced by Liu \textit{et al.} in \cite{liu2016deep}. It includes 26'267 unique vehicles with 221'763 images in total. Each image is attached with an ID label corresponding to the vehicle identity. It is the dataset with one of the biggest unique ID collections and features pictures with different resolutions and quality of visibility as well as vehicles in motion state. Vehicles are mostly seen, however, from the front and the back only. The VeRi-776 introduced by Xinchen Liu \textit{et al.} in \cite{liu2016deep2} contains 49'357 images of 776 unique vehicles from 20 different cameras. Each image is attached with vehicle ID, bounding box, type, color, brand. The dataset makes up for its smaller set of unique id's compared to the VehicleID with better quality pictures, better visibility and vehicles from all angles not just back and front. The CityFlow dataset introduced by Tang \textit{et al.} in \cite{tang2019cityflow} is part of the AI City challenge and is a traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras. The dataset contains 229'680 images and around 700 unique vehicles. The 2020 AI City challenge \cite{aicity2020data} includes this dataset as videos, but also as cropped vehicle images for ReID model training. The quality of images is high and vehicles can be seen from many angles. The difference between this and the previous two widely used datasets is the types of vehicles seen, namely, US American versus Chinese, which creates a great parameter shift.
	
	 \begin{figure}[t]
		\centering
		\includegraphics[width=0.5\textwidth]{re_id_diagramma_2.png} % Adjust width as needed
		\caption{Parameter shift is visible between the (from the left) VehicleID \cite{liu2016deep}, VeRi-776 \cite{liu2016deep2} and CityFlow datasets.}
		\label{fig:fig2} % Label for referencing the image
	\end{figure}
	
	\subsection{Synthetic datasets}
		 
	
	
	
	
	\section{Proposed Method}
%	Describe the steps of your multi-step pipeline for vehicle re-identification. Use equations and figures as needed.
%	\begin{itemize}
%		\item Object Detection
%		\item Feature Extraction
%		\item Vector Embedding with LanceDB
%	\end{itemize}
	
	\section{Experiments and Results}
%	Present the experiments, datasets, and evaluation metrics used to validate the method. Include tables, graphs, and illustrations.
	
	\section{Discussion}
%	Discuss the results and their implications. Highlight the future work on edge device implementation.
	
	\section{Conclusion}
%	Summarize the main findings of the paper and the significance of your work.
	
	% Bibliography
	\bibliographystyle{IEEEtran}
	\bibliography{references} % Make sure you have a .bib file with your references
	
\end{document}
